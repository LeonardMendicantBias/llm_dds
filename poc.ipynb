{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import bz2\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"./ILSVRC2012_val_00003014.JPEG\", cv2.IMREAD_COLOR)\n",
    "img = cv2.resize(img, (224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196, 390])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:119] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 19607777280 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 148\u001b[0m\n\u001b[1;32m    140\u001b[0m \t\t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(sequences, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    142\u001b[0m llm_zip \u001b[38;5;241m=\u001b[39m LLMCompression(\n\u001b[1;32m    143\u001b[0m \t\u001b[38;5;66;03m# llm_name=\"meta-llama/Llama-3.2-1B\",\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \tllm_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsloth/Llama-3.2-1B-bnb-4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    145\u001b[0m \tcontext_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m    146\u001b[0m \tpatch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m    147\u001b[0m )\n\u001b[0;32m--> 148\u001b[0m \u001b[43mllm_zip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 85\u001b[0m, in \u001b[0;36mLLMCompression.encode\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     82\u001b[0m bos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull([tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbos_token_id, device\u001b[38;5;241m=\u001b[39mtokens\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     83\u001b[0m tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((bos, tokens), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m logits \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     87\u001b[0m word_logits \u001b[38;5;241m=\u001b[39m logits[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwords]\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:704\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n\u001b[1;32m    703\u001b[0m slice_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m-\u001b[39mlogits_to_keep, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logits_to_keep, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m logits_to_keep\n\u001b[0;32m--> 704\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:119] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 19607777280 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "class LLMCompression:\n",
    "\n",
    "\tdef __init__(self,\n",
    "\t\tllm_name: str,\n",
    "\t\tcontext_size: int,\n",
    "\t\tpatch_size: int, # context_window: int,\n",
    "\t\tcolor_sep: str=\"-\",\n",
    "\t\tpixel_sep: str=\"|\",\n",
    "\t):\n",
    "\t\tself.llm_name = llm_name\n",
    "\t\tself.llm = AutoModelForCausalLM.from_pretrained(\n",
    "\t\t\tllm_name,\n",
    "\t\t\ttorch_dtype=torch.bfloat16,\n",
    "\t\t\t# load_in_4bit=True,\n",
    "\t\t\t# quantization_config=quantization_config,\n",
    "\t\t\t# device_map=torch.device(\"cuda\"),\n",
    "\t\t\t# device_map=\"auto\"\n",
    "\t\t\tdevice_map=\"cpu\"\n",
    "\t\t)\n",
    "\t\tself.llm.eval()\n",
    "\t\tfor param in self.llm.parameters():\n",
    "\t\t\tparam.requires_grad = False\n",
    "\t\tself.tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "\n",
    "\t\tself.context_size = context_size\n",
    "\t\tself.patch_size = patch_size\n",
    "\t\tself.color_sep = color_sep\n",
    "\t\tself.pixel_sep = pixel_sep\n",
    "\t\tself.words = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '.', self.color_sep, self.pixel_sep]\n",
    "\t\tself.tokens = self.tokenizer(self.words, add_special_tokens=False)\n",
    "\t\tself.tokens = [x for xs in self.tokens[\"input_ids\"] for x in xs]\n",
    "\n",
    "\t\tself.word2token = {w: idx for w, idx in zip(self.words, self.tokens)}\n",
    "\t\tself.token2word = {idx: w for idx, w in zip(self.tokens, self.words)}\n",
    "\t\n",
    "\tdef _pad(self, tokens):\n",
    "\t\t# make sure the input fit at least to 1 context length\n",
    "\t\tif tokens.shape[1] >= self.context_size:\n",
    "\t\t\treturn tokens, torch.zeros(tokens.shape[1], device=tokens.device)\n",
    "\t\tpad_len = self.context_size - tokens.shape[1] % self.context_size\n",
    "\n",
    "\t\tpads = torch.full([pad_len], self.tokenizer.eos_token_id, device=tokens.device).unsqueeze(0)\n",
    "\t\tpadded_tokens = torch.cat([tokens, pads])\n",
    "\n",
    "\t\treturn padded_tokens, pad_len\n",
    "\t\n",
    "\t# rank from \"stable\" sort\n",
    "\tdef _get_rank(self, logits, token_ids):  # (B, N, V) (B, N)\n",
    "\t\t'''\n",
    "\t\t\tFind the rank of the tokens\n",
    "\t\t\t1/ The number of values less than token_ids\n",
    "\t\t\t2/ The number of values equal with token_ids\n",
    "\t\t'''\n",
    "\t\t# count the strictly the number of greater values\n",
    "\t\tselected_logits = logits.gather(-1, token_ids[..., None]).squeeze(-1)\n",
    "\t\tn_gt = (logits > selected_logits[..., None]).sum(-1)  # (B, N)\n",
    "\n",
    "\t\t# \"mimic\" stable sorting\n",
    "\t\teq = logits.eq(selected_logits[..., None])  # (B, N, V)\n",
    "\t\tmask = torch.arange(logits.shape[-1], device=logits.device) < token_ids.unsqueeze(-1)\n",
    "\t\tn_eq = (eq*mask).sum(-1)\n",
    "\n",
    "\t\treturn n_gt + n_eq\n",
    "\t\n",
    "\tdef encode(self, img):\n",
    "\t\tp_size = self.patch_size\n",
    "\n",
    "\t\tpatches = np.array([\n",
    "\t\t\timg[i*p_size:(i+1)*p_size, j*p_size:(j+1)*p_size, :].flatten()\n",
    "\t\t\tfor i in range(img.shape[0]//p_size)\n",
    "\t\t\tfor j in range(img.shape[1]//p_size)\n",
    "\t\t])\n",
    "\n",
    "\t\ttokens = self.patch2tokens(patches)\n",
    "\t\ttokens = tokens[\"input_ids\"].squeeze()\n",
    "\t\ttokens = tokens.to(self.llm.device)\n",
    "\n",
    "\t\t# tokens, pad_len = self._pad(tokens[1:])\n",
    "\t\t# tokens = tokens.view(-1, self.context_size)\n",
    "\n",
    "\t\tbos = torch.full([tokens.shape[0]], self.tokenizer.bos_token_id, device=tokens.device).unsqueeze(1)\n",
    "\t\ttokens = torch.cat((bos, tokens), 1)\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\toutput = self.llm(tokens[:, :-1])\n",
    "\t\t\tlogits = output.logits\n",
    "\t\t\tword_logits = logits[..., self.words]\n",
    "\t\t\tranks = self._get_rank(word_logits, tokens[:, 1:])\n",
    "\n",
    "\t\treturn ranks#, pad_len\n",
    "\n",
    "\tdef decode(self, rank: List[int], pad_len: int):\n",
    "\t\tgenerated_ids = torch.full((rank.shape[0], 1), self.tokenizer.bos_token_id, device=rank.device)\n",
    "\t\t\n",
    "\t\tpast_key_values = None\n",
    "\t\tfor idx in range(self.context_size):\n",
    "\t\t\toutput = self.llm(generated_ids, past_key_values=past_key_values, use_cache=False)\n",
    "\t\t\tpast_key_values = output.past_key_values\n",
    "\n",
    "\t\t\tlogits = output.logits[:, -1, :]  # shape: (n_chunks, vocab)\n",
    "\t\t\tlogits, sorted_tokens = torch.sort(logits, descending=True, stable=True)\n",
    "\n",
    "\t\t\tnext_token_id = sorted_tokens.gather(-1, rank[:, idx].unsqueeze(-1))\n",
    "\n",
    "\t\t\tgenerated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
    "\n",
    "\t\toutput = generated_ids[:, 1:].flatten()\n",
    "\t\treturn self.tokenizer.decode(output[:-pad_len], skip_special_tokens=True)\n",
    "\n",
    "\tdef evaluate(self, s):\n",
    "\t\trank, pad_len = self.encode(s)\n",
    "\t\ttorch.cuda.empty_cache()\n",
    "\n",
    "\t\ts_hat = self.decode(rank, pad_len)\n",
    "\t\tassert s_hat == s, f\"incorrect (de)-compression \\n Expected: {s} \\n Got: {s_hat}\"\n",
    "\n",
    "\t\tcompressed_s = bz2.compress(s.encode('utf-8'))\n",
    "\t\t_rank = rank.flatten()\n",
    "\t\tcompressed_s_hat = bz2.compress(_rank.cpu().numpy().tobytes())\n",
    "\n",
    "\t\t# Get the size of the compressed data\n",
    "\t\ts_size = len(compressed_s)\n",
    "\t\ts_hat_size = len(compressed_s_hat)\n",
    "\t\t# print(s_hat_size, s_size)\n",
    "\t\tprint(f\"Compression ratio: {(s_hat_size / s_size)*100:.4f}\")\n",
    "\n",
    "\t\treturn _rank, pad_len\n",
    "\t\n",
    "\tdef patch2tokens(self, patches):\n",
    "\t\tsequences = [\n",
    "\t\t\tf'{self.pixel_sep}'.join([\n",
    "\t\t\t\t\tf'{self.color_sep}'.join([\n",
    "\t\t\t\t\t\tstr(num)\n",
    "\t\t\t\t\t\tfor num in patch[i*3:(i+1)*3]\n",
    "\t\t\t\t\t])\n",
    "\t\t\t\tfor i in range(len(patches)//3)\n",
    "\t\t\t])\n",
    "\t\t\tfor patch in patches\n",
    "\t\t]\n",
    "\t\treturn self.tokenizer(sequences, return_tensors=\"pt\")\n",
    "\n",
    "llm_zip = LLMCompression(\n",
    "\t# llm_name=\"meta-llama/Llama-3.2-1B\",\n",
    "\tllm_name=\"unsloth/Llama-3.2-1B-bnb-4bit\",\n",
    "\tcontext_size=256,\n",
    "\tpatch_size=16,\n",
    ")\n",
    "llm_zip.encode(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit llm_zip.encode(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit llm_zip.decode(rank, pad_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '.', '-', \"|\"]\n",
    "tokens = llm_zip.tokenizer(words, add_special_tokens=False)\n",
    "tokens = [x for xs in tokens[\"input_ids\"] for x in xs]\n",
    "\n",
    "word2token = {w: idx for w, idx in zip(words, tokens)}\n",
    "token2word = {idx: w for idx, w in zip(tokens, words)}\n",
    "word2token, token2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
