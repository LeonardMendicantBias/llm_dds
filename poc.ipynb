{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import bz2\n",
    "import pprint\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import array\n",
    "import zlib\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "CONTEXT_SIZE = 256\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "ranks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = model_name\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def text_to_tokens(text):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "    tokens = tokens[\"input_ids\"].squeeze()\n",
    "    return tokens\n",
    "\n",
    "def pad(tokens, padding_val):\n",
    "    pad_len = CONTEXT_SIZE - tokens.shape[0] % CONTEXT_SIZE\n",
    "    print(\"pad_len\", pad_len)\n",
    "    if pad_len != CONTEXT_SIZE:\n",
    "        padding = torch.tensor([padding_val]*pad_len)\n",
    "        tokens = torch.cat((tokens, padding))\n",
    "    else:\n",
    "        pad_len = 0\n",
    "\n",
    "    return tokens, pad_len\n",
    "\n",
    "# @torch.no_grad()\n",
    "def get_logits(tokens, token_index, past=None):\n",
    "    my_inputs = {}\n",
    "    my_inputs['input_ids'] = tokens[:, token_index].reshape(-1, 1)\n",
    "\n",
    "    output = model(**my_inputs, past_key_values=past)\n",
    "    logits = output.logits\n",
    "    if len(logits.shape) > 2:\n",
    "        logits = logits.reshape((logits.shape[0], -1))\n",
    "    return logits, output.past_key_values\n",
    "    \n",
    "def encode_one_batch(\n",
    "    tokens,\n",
    "    token_index,\n",
    "    past=None\n",
    "):\n",
    "    assert len(tokens.shape) == 2\n",
    "\n",
    "    logits, past = get_logits(tokens, token_index, past)\n",
    "    assert len(logits.shape) == 2\n",
    "    logits, sorted_tokens = torch.sort(logits, descending=True)\n",
    "\n",
    "    assert len(sorted_tokens.shape) == 2\n",
    "\n",
    "    next_tokens = tokens[:, token_index + 1]\n",
    "    next_tokens_expanded = next_tokens.view(-1, 1).expand_as(sorted_tokens)\n",
    "\n",
    "    # Find score as index of next tokens\n",
    "    scores = (sorted_tokens == next_tokens_expanded).nonzero(as_tuple=True)\n",
    "\n",
    "    scores = scores[1] # remove index column\n",
    "\n",
    "    ranks.extend(scores.tolist())\n",
    "\n",
    "    return scores, past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(scores):\n",
    "    output_tokens = decode_tokens(scores)\n",
    "    text = tokenizer.batch_decode(output_tokens)\n",
    "    text = \"\".join(text)\n",
    "    #text = text.replace(\"<|endoftext|>\", \"\")\n",
    "    return text\n",
    "\n",
    "def decode_tokens(scores):\n",
    "\n",
    "    scores, pad_len = pad(scores, tokenizer.eos_token_id)\n",
    "\n",
    "    scores = scores.view(-1, CONTEXT_SIZE) # all rows, CONTEXT_SIZE\n",
    "\n",
    "    output_tokens = torch.zeros(scores.shape, dtype=int, device=scores.device)\n",
    "\n",
    "    # Add eos to the start of each block (to give it somewhere to start)\n",
    "    eos = torch.tensor([tokenizer.eos_token_id]*output_tokens.shape[0]).unsqueeze(1)\n",
    "    output_tokens = torch.cat((eos, output_tokens), 1) # all rows, CONTEXT_SIZE + 1\n",
    "\n",
    "\n",
    "    batches = scores.shape[0] // BATCH_SIZE\n",
    "    if scores.shape[0] % BATCH_SIZE != 0:\n",
    "        batches += 1\n",
    "\n",
    "    # score each batch\n",
    "    print(\"Decoding\")\n",
    "    for i in range(batches):\n",
    "        print(i, \"out of\", batches)\n",
    "        cur_scores = scores[i*BATCH_SIZE:(i + 1)*BATCH_SIZE] # BATCH_SIZE, CONTEXT_SIZE\n",
    "\n",
    "        cur_output_tokens = output_tokens[i*BATCH_SIZE:(i + 1)*BATCH_SIZE] # BATCH_SIZE, CONTEXT_SIZE\n",
    "        \n",
    "        past = None\n",
    "        for j in tqdm(range(scores.shape[1])):\n",
    "\n",
    "            cur_output_tokens[:, j+1], past = decode_one_batch(cur_output_tokens, cur_scores, j, past) # BATCH_SIZE\n",
    "\n",
    "        output_tokens[i*BATCH_SIZE:(i + 1)*BATCH_SIZE] = cur_output_tokens\n",
    "\n",
    "    output_tokens = output_tokens[:, 1:].int()\n",
    "    output_tokens = output_tokens.flatten()\n",
    "\n",
    "    if pad_len != 0:\n",
    "        output_tokens = output_tokens[:-pad_len]\n",
    "\n",
    "    return output_tokens\n",
    "\n",
    "def decode_one_batch(input_tokens, scores, score_index, past=None):\n",
    "    assert len(scores.shape) == 2\n",
    "    logits, past = get_logits(input_tokens, score_index, past)\n",
    "\n",
    "    logits, sorted_tokens = torch.sort(logits, descending=True)\n",
    "    assert len(sorted_tokens.shape) == 2\n",
    "    # the scores give the indexes of the decoded tokens\n",
    "    indexes = scores[:, score_index].flatten()\n",
    "    decoded_tokens = sorted_tokens[torch.arange(indexes.shape[0]), indexes]\n",
    "\n",
    "    return decoded_tokens.int(), past\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String length: 3796\n"
     ]
    }
   ],
   "source": [
    "s = \"Artificial intelligence (AI) has undergone a remarkable transformation over the past few decades, revolutionizing various industries and fundamentally changing the way humans interact with technology. The journey of AI began with early theoretical concepts in the 1950s, when pioneers like Alan Turing proposed the idea of intelligent machines capable of reasoning and problem-solving. Over the years, advancements in computing power, data availability, and algorithmic innovation have fueled AI's rapid progress, leading to the development of sophisticated models that can perform complex tasks with remarkable accuracy. One of the most significant applications of AI is in healthcare. AI-powered systems assist medical professionals in diagnosing diseases, predicting patient outcomes, and recommending personalized treatment plans. Machine learning models trained on vast datasets of medical images can detect anomalies such as tumors in X-rays, MRIs, and CT scans with high precision. Natural language processing (NLP) enables AI chatbots and virtual assistants to interact with patients, answering common health-related queries and reducing the burden on healthcare providers. Furthermore, AI-driven drug discovery accelerates the development of new treatments by analyzing molecular structures and predicting their potential efficacy. These innovations not only enhance patient care but also contribute to reducing healthcare costs and improving overall efficiency in the medical sector. The financial industry has also been profoundly impacted by AI. Banks and investment firms leverage AI-driven algorithms to detect fraudulent transactions, assess credit risk, and optimize trading strategies. High-frequency trading systems utilize machine learning to analyze market trends and execute trades at speeds beyond human capability. Personalized financial assistants powered by AI provide users with tailored investment advice based on their financial history, risk tolerance, and goals. However, AI in finance is not without challengesâ€”bias in algorithms, data privacy concerns, and the risk of over-reliance on automated decision-making remain critical issues that regulators and institutions must address to ensure fair and transparent financial practices. In the realm of education, AI is reshaping traditional teaching methods. Adaptive learning platforms use AI to tailor educational content to individual students based on their learning pace, strengths, and weaknesses. Automated grading systems reduce the workload of teachers by efficiently evaluating assignments and exams. AI-powered language translation tools help break down language barriers, making quality education accessible to students worldwide. However, the integration of AI in education also raises concerns about data privacy, algorithmic fairness, and the potential loss of human interaction in the learning process. Striking a balance between technological advancement and pedagogical effectiveness is crucial to maximizing the benefits of AI in education. AI's influence extends to entertainment and media as well. Streaming platforms such as Netflix, Spotify, and YouTube utilize AI-driven recommendation systems to personalize content for users. These algorithms analyze viewing, listening, and browsing histories to suggest movies, music, and articles that align with user preferences. AI-generated content, including deepfake videos, music compositions, and even articles, demonstrates the potential of machine learning in creative fields. While AI enhances user experience and content curation, it also raises ethical concerns regarding misinformation, deepfake manipulation, and intellectual property rights. Ensuring responsible AI use in the media industry is crucial to maintaining trust and credibility.\"\n",
    "\n",
    "print(\"String length:\", len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 635\n",
      "pad_len 133\n",
      "Encoding\n",
      "0 out of 1\n",
      "------------------------------\n",
      "tensor([ 8001, 20755,   290])\n",
      "torch.Size([3, 50257])\n",
      "------------------------------\n",
      "(tensor([0, 1, 2]), tensor([  289, 45802,    87]))\n",
      "------------------------------\n",
      "tensor([9542,  416, 2056])\n",
      "torch.Size([3, 50257])\n",
      "------------------------------\n",
      "(tensor([0, 1, 2]), tensor([   1,    0, 1150]))\n",
      "------------------------------\n",
      "tensor([4430, 9552,  355])\n",
      "torch.Size([3, 50257])\n",
      "------------------------------\n",
      "(tensor([0, 1, 2]), tensor([  0, 563,  82]))\n"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "\n",
    "# generate the next token\n",
    "def get_logits(tokens, token_index, past=None):\n",
    "    my_inputs = {}\n",
    "    my_inputs['input_ids'] = tokens[:, token_index].reshape(-1, 1)\n",
    "\n",
    "    output = model(**my_inputs, past_key_values=past)\n",
    "    logits = output.logits\n",
    "    if len(logits.shape) > 2:\n",
    "        logits = logits.reshape((logits.shape[0], -1))\n",
    "    return logits, output.past_key_values\n",
    "\n",
    "# %timeit get_rank(probas, targets)\n",
    "\n",
    "def encode_one_batch(\n",
    "    tokens,\n",
    "    token_index,\n",
    "    past=None\n",
    "):\n",
    "    assert len(tokens.shape) == 2\n",
    "\n",
    "    logits, past = get_logits(tokens, token_index, past)\n",
    "    assert len(logits.shape) == 2\n",
    "    sorted_logits, sorted_tokens = torch.sort(logits, descending=True)\n",
    "\n",
    "    assert len(sorted_tokens.shape) == 2\n",
    "\n",
    "    next_tokens = tokens[:, token_index + 1]\n",
    "    print(\"-\"*30)\n",
    "    print(next_tokens)\n",
    "    next_tokens_expanded = next_tokens.view(-1, 1).expand_as(sorted_tokens)\n",
    "    print(next_tokens_expanded.shape)\n",
    "    print(\"-\"*30)\n",
    "\n",
    "    # the indices that match the next token\n",
    "    scores = (sorted_tokens == next_tokens_expanded).nonzero(as_tuple=True)\n",
    "    print(scores)\n",
    "\n",
    "    scores = scores[1] # remove index column\n",
    "\n",
    "    ranks.extend(scores.tolist())\n",
    "\n",
    "    return scores, past\n",
    "\n",
    "# tokenization\n",
    "tokens = text_to_tokens(s)\n",
    "print(\"tokens length:\", len(tokens))\n",
    "\n",
    "# padding\n",
    "tokens, pad_len = pad(tokens, tokenizer.eos_token_id)\n",
    "tokens = tokens.view(-1, CONTEXT_SIZE)\n",
    "\n",
    "output_scores = torch.zeros(tokens.shape)\n",
    "\n",
    "eos = torch.tensor([tokenizer.eos_token_id]*tokens.shape[0]).unsqueeze(1)\n",
    "tokens = torch.cat((eos, tokens), 1)\n",
    "\n",
    "batches = tokens.shape[0] // BATCH_SIZE\n",
    "if tokens.shape[0] % BATCH_SIZE != 0:\n",
    "    batches += 1\n",
    "\n",
    "# score each batch\n",
    "print(\"Encoding\")\n",
    "\n",
    "# mini-chunks\n",
    "with torch.no_grad():\n",
    "    for i in range(batches):\n",
    "        print(i, \"out of\", batches)\n",
    "\n",
    "        cur_tokens = tokens[i*BATCH_SIZE:(i + 1)*BATCH_SIZE]  # (n_chunks, n_tokens)\n",
    "        cur_output_scores = torch.zeros((cur_tokens.shape[0], cur_tokens.shape[1] - 1))  # -1 for auto-regressive\n",
    "        past = None\n",
    "\n",
    "        # print(cur_tokens)\n",
    "        for j in range(cur_tokens.shape[1] - 1):\n",
    "            cur_output_scores[:, j], past = encode_one_batch(cur_tokens, j, past)\n",
    "            if j == 2: break\n",
    "        \n",
    "        output_scores[i*BATCH_SIZE:(i + 1)*BATCH_SIZE] = cur_output_scores\n",
    "        del cur_tokens\n",
    "        break\n",
    "\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "    # output_scores = output_scores.flatten().int()\n",
    "    # if pad_len > 0:\n",
    "    #     output_scores = output_scores[:-pad_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_bytes = output_scores.numpy().tobytes()\n",
    "\n",
    "# Compress the tensor bytes using bz2\n",
    "compressed_bytes = bz2.compress(tensor_bytes)\n",
    "with open('./compressed.gpz', \"wb\") as f:\n",
    "    f.write(compressed_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./compressed.gpz', \"rb\") as f:\n",
    "    zipped = f.read()\n",
    "unzipped = bz2.decompress(zipped)\n",
    "unzipped = array.array(\"H\", unzipped)\n",
    "decoded = decode(torch.tensor(unzipped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
