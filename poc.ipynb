{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import bz2\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LLMCompression:\n",
    "\n",
    "\tdef __init__(self,\n",
    "\t\tllm_name: str,\n",
    "\t\tcontext_size: int,\n",
    "\t\tpatch_size: int, # context_window: int,\n",
    "\t\tcolor_sep: str=\"-\",\n",
    "\t\tpixel_sep: str=\"|\",\n",
    "\t):\n",
    "\t\tself.llm_name = llm_name\n",
    "\t\tself.llm = AutoModelForCausalLM.from_pretrained(\n",
    "\t\t\tllm_name,\n",
    "\t\t\ttorch_dtype=torch.bfloat16,\n",
    "\t\t\t# load_in_4bit=True,\n",
    "\t\t\t# quantization_config=quantization_config,\n",
    "\t\t\t# device_map=torch.device(\"cuda\"),\n",
    "\t\t\tdevice_map=\"auto\"\n",
    "\t\t)\n",
    "\t\tself.llm.eval()\n",
    "\t\tfor param in self.llm.parameters():\n",
    "\t\t\tparam.requires_grad = False\n",
    "\t\tself.tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "\n",
    "\t\tself.context_size = context_size\n",
    "\t\tself.patch_size = patch_size\n",
    "\t\tself.color_sep = color_sep\n",
    "\t\tself.pixel_sep = pixel_sep\n",
    "\t\tself.words = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '.', self.color_sep, self.pixel_sep]\n",
    "\t\tself.tokens = self.tokenizer(self.words, add_special_tokens=False)\n",
    "\t\tself.tokens = [x for xs in self.tokens[\"input_ids\"] for x in xs]\n",
    "\n",
    "\t\tself.word2token = {w: idx for w, idx in zip(self.words, self.tokens)}\n",
    "\t\tself.token2word = {idx: w for idx, w in zip(self.tokens, self.words)}\n",
    "\t\n",
    "\tdef _pad(self, tokens):\n",
    "\t\tif tokens.shape[0] % self.context_size == 0:\n",
    "\t\t\treturn tokens, torch.zeros(tokens.shape[0], device=tokens.device)\n",
    "\t\tpad_len = self.context_size - tokens.shape[0] % self.context_size\n",
    "\n",
    "\t\tpads = torch.full([pad_len], self.tokenizer.eos_token_id, device=tokens.device)\n",
    "\t\tpadded_tokens = torch.cat([tokens, pads])\n",
    "\n",
    "\t\treturn padded_tokens, pad_len\n",
    "\t\n",
    "\t# rank from \"stable\" sort\n",
    "\tdef _get_rank(self, logits, token_ids):  # (B, N, V) (B, N)\n",
    "\t\t'''\n",
    "\t\t\tFind the rank of the tokens\n",
    "\t\t\t1/ The number of values less than token_ids\n",
    "\t\t\t2/ The number of values equal with token_ids\n",
    "\t\t'''\n",
    "\t\t# count the strictly the number of greater values\n",
    "\t\tselected_logits = logits.gather(-1, token_ids[..., None]).squeeze(-1)\n",
    "\t\tn_gt = (logits > selected_logits[..., None]).sum(-1)  # (B, N)\n",
    "\n",
    "\t\t# \"mimic\" stable sorting\n",
    "\t\teq = logits.eq(selected_logits[..., None])  # (B, N, V)\n",
    "\t\tmask = torch.arange(logits.shape[-1], device=logits.device) < token_ids.unsqueeze(-1)\n",
    "\t\tn_eq = (eq*mask).sum(-1)\n",
    "\n",
    "\t\treturn n_gt + n_eq\n",
    "\n",
    "\tdef encode(self, s):\n",
    "\t\ttokens = self.tokenizer(s, return_tensors=\"pt\")\n",
    "\t\ttokens = tokens[\"input_ids\"].squeeze()\n",
    "\t\ttokens = tokens.to(self.llm.device)\n",
    "\n",
    "\t\ttokens, pad_len = self._pad(tokens[1:])\n",
    "\t\ttokens = tokens.view(-1, self.context_size)\n",
    "\n",
    "\t\tbos = torch.full([tokens.shape[0]], self.tokenizer.bos_token_id, device=tokens.device).unsqueeze(1)\n",
    "\t\ttokens = torch.cat((bos, tokens), 1)\n",
    "\n",
    "\t\toutput = self.llm(tokens[:, :-1])\n",
    "\t\tlogits = output.logits\n",
    "\t\tword_logits = logits[..., self.words]\n",
    "\t\tranks = self._get_rank(word_logits, tokens[:, 1:])\n",
    "\n",
    "\t\treturn ranks, pad_len\n",
    "\t\n",
    "\tdef encode(self, img):\n",
    "\t\tp_size = self.patch_size\n",
    "\t\tpatches = np.array([\n",
    "\t\t\timg[i*p_size:(i+1)*p_size, j*p_size:(j+1)*p_size, :].flatten()\n",
    "\t\t\tfor i in range(img.shape[0]//p_size)\n",
    "\t\t\tfor j in range(img.shape[1]//p_size)\n",
    "\t\t])\n",
    "\t\ttokens = self.tokenizer(s, return_tensors=\"pt\")\n",
    "\t\ttokens = tokens[\"input_ids\"].squeeze()\n",
    "\t\ttokens = tokens.to(self.llm.device)\n",
    "\n",
    "\t\ttokens, pad_len = self._pad(tokens[1:])\n",
    "\t\ttokens = tokens.view(-1, self.context_size)\n",
    "\n",
    "\t\tbos = torch.full([tokens.shape[0]], self.tokenizer.bos_token_id, device=tokens.device).unsqueeze(1)\n",
    "\t\ttokens = torch.cat((bos, tokens), 1)\n",
    "\n",
    "\t\toutput = self.llm(tokens[:, :-1])\n",
    "\t\tlogits = output.logits\n",
    "\t\tword_logits = logits[..., self.words]\n",
    "\t\tranks = self._get_rank(word_logits, tokens[:, 1:])\n",
    "\n",
    "\t\treturn ranks, pad_len\n",
    "\n",
    "\n",
    "\tdef decode(self, rank: List[int], pad_len: int):\n",
    "\t\tgenerated_ids = torch.full((rank.shape[0], 1), self.tokenizer.bos_token_id, device=rank.device)\n",
    "\t\t\n",
    "\t\tpast_key_values = None\n",
    "\t\tfor idx in range(self.context_size):\n",
    "\t\t\toutput = self.llm(generated_ids, past_key_values=past_key_values, use_cache=False)\n",
    "\t\t\tpast_key_values = output.past_key_values\n",
    "\n",
    "\t\t\tlogits = output.logits[:, -1, :]  # shape: (n_chunks, vocab)\n",
    "\t\t\tlogits, sorted_tokens = torch.sort(logits, descending=True, stable=True)\n",
    "\n",
    "\t\t\tnext_token_id = sorted_tokens.gather(-1, rank[:, idx].unsqueeze(-1))\n",
    "\n",
    "\t\t\tgenerated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
    "\n",
    "\t\toutput = generated_ids[:, 1:].flatten()\n",
    "\t\treturn self.tokenizer.decode(output[:-pad_len], skip_special_tokens=True)\n",
    "\n",
    "\tdef evaluate(self, s):\n",
    "\t\trank, pad_len = self.encode(s)\n",
    "\t\ttorch.cuda.empty_cache()\n",
    "\n",
    "\t\ts_hat = self.decode(rank, pad_len)\n",
    "\t\tassert s_hat == s, f\"incorrect (de)-compression \\n Expected: {s} \\n Got: {s_hat}\"\n",
    "\n",
    "\t\tcompressed_s = bz2.compress(s.encode('utf-8'))\n",
    "\t\t_rank = rank.flatten()\n",
    "\t\tcompressed_s_hat = bz2.compress(_rank.cpu().numpy().tobytes())\n",
    "\n",
    "\t\t# Get the size of the compressed data\n",
    "\t\ts_size = len(compressed_s)\n",
    "\t\ts_hat_size = len(compressed_s_hat)\n",
    "\t\t# print(s_hat_size, s_size)\n",
    "\t\tprint(f\"Compression ratio: {(s_hat_size / s_size)*100:.4f}\")\n",
    "\n",
    "\t\treturn _rank, pad_len\n",
    "\t\n",
    "\tdef patch2tokens(self, patches):\n",
    "\t\tsequences = [\n",
    "\t\t\tf'{self.pixel_sep}'.join([\n",
    "\t\t\t\t\tf'{self.color_sep}'.join([\n",
    "\t\t\t\t\t\tstr(num)\n",
    "\t\t\t\t\t\tfor num in patch[i*3:(i+1)*3]\n",
    "\t\t\t\t\t])\n",
    "\t\t\t\tfor i in range(len(patches)//3)\n",
    "\t\t\t])\n",
    "\t\t\tfor patch in patches\n",
    "\t\t]\n",
    "\t\treturn self.tokenizer(sequences, return_tensors=\"pt\")\n",
    "\n",
    "llm_zip = LLMCompression(\n",
    "\t# llm_name=\"meta-llama/Llama-3.2-1B\",\n",
    "\tllm_name=\"unsloth/Llama-3.2-1B-bnb-4bit\",\n",
    "\tcontext_size=256,\n",
    "\tpatch_size=16,\n",
    ")\n",
    "\n",
    "img = cv2.imread(\"./ILSVRC2012_val_00003014.JPEG\", cv2.IMREAD_COLOR)\n",
    "img = cv2.resize(img, (224, 224))\n",
    "# img = transform(img)\n",
    "\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_size = 16\n",
    "patches = np.array([\n",
    "    img[i*p_size:(i+1)*p_size, j*p_size:(j+1)*p_size, :].flatten()\n",
    "    for i in range(img.shape[0]//p_size)\n",
    "    for j in range(img.shape[1]//p_size)\n",
    "])\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7-53-17|10-70-32|8-33-6|8-116-43|9-96-32|4-29-7|4-12-3|4-36-10|4-37-8|5-48-10|6-11-6|80-104-107|16-34-17|7-19-7|7-19-7|4-8-3|6-58-22|5-46-10|14-38-18|6-74-26|11-69-15|14-39-15|13-18-9|0-37-8|3-37-7|8-32-8|13-21-14|4-27-24|14-108-51|11-37-22|5-24-3|4-15-4|15-64-23|15-52-25|7-39-11|5-25-6|20-85-42|10-77-32|0-17-4|4-37-11|2-43-10|9-28-7|9-24-2|7-6-6|30-83-45|40-67-40|7-19-9|18-33-8|4-57-21|6-29-9|6-23-7|5-17-4|7-5-3|28-137-70|5-7-3|2-46-9|8-43-14|9-25-6|10-23-6|10-29-9|41-60-53|106-175-121|103-154-109|16-35-17|15-56-21'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = [\n",
    "    f'{llm_zip.pixel_sep}'.join([\n",
    "            f'{llm_zip.color_sep}'.join([\n",
    "                str(num)\n",
    "                for num in patch[i*3:(i+1)*3]\n",
    "            ])\n",
    "        for i in range(len(patches)//3)\n",
    "    ])\n",
    "    for patch in patches\n",
    "]\n",
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196, 390])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc = llm_zip.tokenizer(sequences, return_tensors=\"pt\")\n",
    "abc[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sequence in sequences:\n",
    "    tokens = llm_zip.tokenizer(sequence)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for patch in patches:\n",
    "    for i in range(len(patches)//3):\n",
    "        s = \"-\".join([str(num) for num in patch[i*3:(i+1)*3]])\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape[0]//p_size, img.shape[1]//p_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches[0, :20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_img = cv2.imread(\"./ILSVRC2012_val_00003014.JPEG\", cv2.IMREAD_COLOR)\n",
    "_img = cv2.resize(_img, (224, 224))\n",
    "_img[0, 0], _img[0, 1], _img[0, 2], _img[0, 3], _img[1, 0], _img[1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit llm_zip.encode(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit llm_zip.decode(rank, pad_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '.', '-', \"|\"]\n",
    "tokens = llm_zip.tokenizer(words, add_special_tokens=False)\n",
    "tokens = [x for xs in tokens[\"input_ids\"] for x in xs]\n",
    "\n",
    "word2token = {w: idx for w, idx in zip(words, tokens)}\n",
    "token2word = {idx: w for idx, w in zip(tokens, words)}\n",
    "word2token, token2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
