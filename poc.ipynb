{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from numpy import random\n",
    "import bz2\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quant_config,\n",
    "    # device_map=torch.device(\"cuda\"),\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "llm.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMCompression:\n",
    "\n",
    "    def __init__(self,\n",
    "        llm_name: str,\n",
    "        context_size: int, # context_window: int,\n",
    "    ):\n",
    "        self.llm_name = llm_name\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,  # Use bfloat16 for better performance\n",
    "            bnb_4bit_use_double_quant=True,  # Double quantization for memory efficiency\n",
    "        )\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(\n",
    "            llm_name,\n",
    "            quantization_config=quant_config,\n",
    "            # device_map=torch.device(\"cuda\"),\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self.llm.eval()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "\n",
    "        self.context_size = context_size\n",
    "    \n",
    "    def _pad(self, tokens):\n",
    "        if tokens.shape[0] % self.context_size == 0:\n",
    "            return tokens, torch.zeros(tokens.shape[0], device=tokens.device)\n",
    "        pad_len = self.context_size - tokens.shape[0] % self.context_size\n",
    "\n",
    "        pads = torch.full([pad_len], self.tokenizer.eos_token_id, device=tokens.device)\n",
    "        padded_tokens = torch.cat([tokens, pads])\n",
    "\n",
    "        return padded_tokens, pad_len\n",
    "    \n",
    "    def _get_rank(self, logits, token_ids):\n",
    "        # count the strictly the number of greater values\n",
    "        selected_logits = logits.gather(-1, token_ids[..., None]).squeeze(-1)\n",
    "        n_gt = (logits > selected_logits[..., None]).sum(-1)\n",
    "\n",
    "        # \"mimic\" stable sorting\n",
    "        eq = (logits == selected_logits[..., None])\n",
    "        mask = torch.arange(logits.shape[-1], device=logits.device).unsqueeze(0) < token_ids.unsqueeze(1)\n",
    "        n_eq = (eq*mask).sum(-1)\n",
    "\n",
    "        return n_gt + n_eq\n",
    "    \n",
    "    def argsort_solution(self, logits, targets):\n",
    "        sort = torch.argsort(-logits, -1)\n",
    "        return torch.where(sort == targets[:, None])[1]\n",
    "\n",
    "    @torch.no_grad\n",
    "    def encode(self, s):\n",
    "        tokens = self.tokenizer(s, return_tensors=\"pt\")\n",
    "        tokens = tokens[\"input_ids\"].squeeze()\n",
    "        tokens = tokens.to(self.llm.device)\n",
    "\n",
    "        tokens, pad_len = self._pad(tokens[1:])\n",
    "        tokens = tokens.view(-1, self.context_size)\n",
    "\n",
    "        bos = torch.full([tokens.shape[0]], self.tokenizer.bos_token_id, device=tokens.device).unsqueeze(1)\n",
    "        tokens = torch.cat((bos, tokens), 1)\n",
    "\n",
    "        ranks = torch.empty_like(tokens[:, :-1])\n",
    "        past_key_values = None\n",
    "        for idx in range(self.context_size):\n",
    "            next_tokens = self.llm(tokens[:, :idx+1], past_key_values=past_key_values)\n",
    "            past_key_values = next_tokens.past_key_values\n",
    "            \n",
    "            rank = self.argsort_solution(next_tokens.logits[:, -1, :], tokens[:, idx+1])\n",
    "            ranks[:, idx] = rank\n",
    "\n",
    "        return ranks, pad_len\n",
    "\n",
    "    @torch.no_grad\n",
    "    def decode(self, rank: List[int], pad_len: int):\n",
    "        generated_ids = torch.full((rank.shape[0], 1), tokenizer.bos_token_id, device=rank.device)\n",
    "        \n",
    "        past_key_values = None\n",
    "        for idx in range(self.context_size):\n",
    "            output = self.llm(generated_ids, past_key_values=past_key_values)\n",
    "            past_key_values = output.past_key_values\n",
    "\n",
    "            logits = output.logits[:, -1, :]  # shape: (n_chunks, vocab)\n",
    "            logits, sorted_tokens = torch.sort(logits, descending=True, stable=True)\n",
    "\n",
    "            next_token_id = sorted_tokens.gather(-1, rank[:, idx].unsqueeze(-1))\n",
    "\n",
    "            generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
    "\n",
    "        output = generated_ids[:, 1:].flatten()\n",
    "        return tokenizer.decode(output[:-pad_len], skip_special_tokens=True)\n",
    "\n",
    "    def evaluate(self, s):\n",
    "        rank, pad_len = self.encode(s)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        s_hat = self.decode(rank, pad_len)\n",
    "        assert s_hat == s, \"incorrect (de)-compression\"\n",
    "\n",
    "        compressed_s = bz2.compress(s.encode('utf-8'))\n",
    "        _rank = rank.flatten()\n",
    "        print(_rank)\n",
    "        compressed_s_hat = bz2.compress(_rank.cpu().numpy().tobytes())\n",
    "\n",
    "        # Get the size of the compressed data\n",
    "        s_size = len(compressed_s)\n",
    "        s_hat_size = len(compressed_s_hat)\n",
    "        print(s_hat_size, s_size)\n",
    "        print(f\"Compression ratio is: {(s_hat_size / s_size)*100:.4f}\")\n",
    "\n",
    "        return _rank, pad_len\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_zip = LLMCompression(\n",
    "    llm_name=\"meta-llama/Llama-3.2-1B\",\n",
    "    context_size=256\n",
    ")\n",
    "# s = \"The rapid advancement of technology has dramatically reshaped the way humans live, work, and interact with the world. In just a few decades, society has transitioned from relying on traditional forms of communication, such as letters and landline telephones, to an era dominated by smartphones, social media, and artificial intelligence. This transformation has brought numerous benefits, making information more accessible, improving efficiency in various industries, and enhancing global connectivity. However, it has also introduced new challenges, including privacy concerns, the digital divide, and the potential for job displacement due to automation. One of the most significant changes driven by technology is the way people communicate. In the past, communication was often slow and limited to physical mail, face-to-face conversations, or expensive long-distance phone calls. Today, instant messaging, video conferencing, and social media platforms allow individuals to stay connected regardless of geographic location. This has strengthened personal relationships, enabled remote work opportunities, and facilitated the exchange of ideas on a global scale. However, the convenience of digital communication has also led to a decline in face-to-face interactions, raising concerns about its impact on social skills and mental health. Moreover, the rise of misinformation and the spread of fake news through digital platforms pose a significant challenge in today's interconnected world. The ability to share information instantly means that false narratives can gain traction quickly, influencing public opinion and even political outcomes. While technology companies have implemented algorithms and fact-checking mechanisms to combat misinformation, the responsibility ultimately lies with users to critically evaluate the information they consume and share. The integration of artificial intelligence and automation has also transformed various industries, improving productivity and efficiency. In healthcare, AI-powered diagnostic tools assist doctors in identifying diseases more accurately, while robotic surgeries enable precision procedures. In the business sector, automation streamlines supply chains, enhances customer service through chatbots, and improves decision-making with data-driven insights. Despite these advantages, the increasing reliance on technology raises concerns about job displacement, as automation continues to replace human workers in certain roles. This shift necessitates a focus on reskilling and upskilling workers to prepare them for the evolving job market. Education systems must adapt to equip students with the skills needed for the digital age, including proficiency in coding, data analysis, and critical thinking. Additionally, ethical considerations surrounding artificial intelligence must be addressed, ensuring that AI systems are developed and used responsibly. Cybersecurity is another pressing issue in the digital era. With the rise of online transactions, cloud computing, and interconnected devices, cyber threats have become more sophisticated. Data breaches, hacking attempts, and identity theft pose risks to individuals and organizations alike. As a result, cybersecurity measures must continually evolve to protect sensitive information and maintain trust in digital platforms. While technology has undoubtedly improved many aspects of life, it is essential to strike a balance between embracing innovation and addressing its challenges. Responsible use, ethical considerations, and continued education will play a crucial role in shaping a future where technology serves humanity in a positive and sustainable manner.\"\n",
    "s = \":\".join(\n",
    "    str(x)\n",
    "    for x in random.randint(0, 5000, (50,)).tolist()\n",
    "    # for x in random.rand(25).tolist()\n",
    ")\n",
    "rank, pad_len = llm_zip.evaluate(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"The rapid advancement of technology has dramatically reshaped the way humans live, work, and interact with the world. In just a few decades, society has transitioned from relying on traditional forms of communication, such as letters and landline telephones, to an era dominated by smartphones, social media, and artificial intelligence. This transformation has brought numerous benefits, making information more accessible, improving efficiency in various industries, and enhancing global connectivity. However, it has also introduced new challenges, including privacy concerns, the digital divide, and the potential for job displacement due to automation. One of the most significant changes driven by technology is the way people communicate. In the past, communication was often slow and limited to physical mail, face-to-face conversations, or expensive long-distance phone calls. Today, instant messaging, video conferencing, and social media platforms allow individuals to stay connected regardless of geographic location. This has strengthened personal relationships, enabled remote work opportunities, and facilitated the exchange of ideas on a global scale. However, the convenience of digital communication has also led to a decline in face-to-face interactions, raising concerns about its impact on social skills and mental health. Moreover, the rise of misinformation and the spread of fake news through digital platforms pose a significant challenge in today's interconnected world. The ability to share information instantly means that false narratives can gain traction quickly, influencing public opinion and even political outcomes. While technology companies have implemented algorithms and fact-checking mechanisms to combat misinformation, the responsibility ultimately lies with users to critically evaluate the information they consume and share. The integration of artificial intelligence and automation has also transformed various industries, improving productivity and efficiency. In healthcare, AI-powered diagnostic tools assist doctors in identifying diseases more accurately, while robotic surgeries enable precision procedures. In the business sector, automation streamlines supply chains, enhances customer service through chatbots, and improves decision-making with data-driven insights. Despite these advantages, the increasing reliance on technology raises concerns about job displacement, as automation continues to replace human workers in certain roles. This shift necessitates a focus on reskilling and upskilling workers to prepare them for the evolving job market. Education systems must adapt to equip students with the skills needed for the digital age, including proficiency in coding, data analysis, and critical thinking. Additionally, ethical considerations surrounding artificial intelligence must be addressed, ensuring that AI systems are developed and used responsibly. Cybersecurity is another pressing issue in the digital era. With the rise of online transactions, cloud computing, and interconnected devices, cyber threats have become more sophisticated. Data breaches, hacking attempts, and identity theft pose risks to individuals and organizations alike. As a result, cybersecurity measures must continually evolve to protect sensitive information and maintain trust in digital platforms. While technology has undoubtedly improved many aspects of life, it is essential to strike a balance between embracing innovation and addressing its challenges. Responsible use, ethical considerations, and continued education will play a crucial role in shaping a future where technology serves humanity in a positive and sustainable manner.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1287 1677\n",
    "Compression ratio is: 76.7442"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "compressed_s = bz2.compress(s.encode('utf-8'))\n",
    "\n",
    "compressed_s_hat = bz2.compress(rank.cpu().numpy().tobytes())\n",
    "\n",
    "print(len(compressed_s_hat), len(compressed_s))\n",
    "print(f\"Compression ratio is: {(len(compressed_s_hat) / len(compressed_s))*100:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# s = \":\".join(\n",
    "#     str(x)\n",
    "#     # for x in random.randint(0, 5000, (50,)).tolist()\n",
    "#     for x in random.rand(25).tolist()\n",
    "# )\n",
    "\n",
    "print(\"String length:\", len(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(tokens, padding_val):\n",
    "    if tokens.shape[0] % CONTEXT_SIZE == 0:\n",
    "        return tokens, torch.zeros(tokens.shape[0], device=tokens.device)\n",
    "    pad_len = CONTEXT_SIZE - tokens.shape[0] % CONTEXT_SIZE\n",
    "\n",
    "    pads = torch.full([pad_len], padding_val, device=tokens.device)\n",
    "    padded_tokens = torch.cat([tokens, pads])\n",
    "\n",
    "    return padded_tokens, pad_len\n",
    "\n",
    "def text_to_tokens(text):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "    return tokens[\"input_ids\"].squeeze()\n",
    "\n",
    "def get_rank(logits, indices):\n",
    "    # count the strictly the number of greater values\n",
    "    selected_logits = logits.gather(-1, indices[..., None]).squeeze(-1)\n",
    "    n_gt = (logits > selected_logits[..., None]).sum(-1)\n",
    "\n",
    "    # \"mimic\" stable sorting\n",
    "    eq = (logits == selected_logits[..., None])#.sum(-1)\n",
    "    mask = torch.arange(logits.shape[-1], device=logits.device).unsqueeze(0) < indices.unsqueeze(1)\n",
    "    n_eq = (eq*mask).sum(-1)\n",
    "\n",
    "    return n_gt + n_eq\n",
    "\n",
    "def argsort_solution(logits, targets):\n",
    "    sort = torch.argsort(-logits, -1)\n",
    "    return torch.where(sort == targets[:, None])[1]\n",
    "\n",
    "def get_token_by_rank(logits, ranks): ...\n",
    "\n",
    "CONTEXT_SIZE = 8\n",
    "s = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# s = \":\".join(\n",
    "#     str(x)\n",
    "#     # for x in random.randint(0, 5000, (50,)).tolist()\n",
    "#     for x in random.rand(25).tolist()\n",
    "# )\n",
    "\n",
    "print(\"String length:\", len(s))\n",
    "\n",
    "tokens = text_to_tokens(s) \n",
    "\n",
    "tokens, pad_len = pad(tokens[1:], tokenizer.eos_token_id)\n",
    "tokens = tokens.view(-1, CONTEXT_SIZE)\n",
    "\n",
    "bos = torch.full([tokens.shape[0]], tokenizer.bos_token_id, device=tokens.device).unsqueeze(1)\n",
    "tokens = torch.cat((bos, tokens), 1)\n",
    "\n",
    "ranks = torch.empty_like(tokens[:, :-1])\n",
    "past_key_values = None\n",
    "for idx in range(CONTEXT_SIZE):\n",
    "    next_tokens = llm(tokens[:, :idx+1].cuda(), past_key_values=past_key_values)\n",
    "    past_key_values = next_tokens.past_key_values\n",
    "    \n",
    "    rank = get_rank(next_tokens.logits[:, -1, :], tokens[:, idx+1].cuda())\n",
    "    # rank = argsort_solution(next_tokens.logits[:, -1, :], tokens[:, idx+1].cuda())\n",
    "    ranks[:, idx] = rank\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(tokens.shape, ranks.shape)\n",
    "generated_ids = torch.tensor([[tokenizer.bos_token_id]]*ranks.shape[0], device=tokens.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    past_key_values = None\n",
    "    for idx in range(CONTEXT_SIZE):\n",
    "        # print(f'\\r{idx}/{CONTEXT_SIZE}', end='')\n",
    "        output = llm(generated_ids.cuda(), past_key_values=past_key_values, top_k=1)\n",
    "        past_key_values = output.past_key_values\n",
    "\n",
    "        logits = output.logits[:, -1, :]  # shape: (n_chunks, vocab)\n",
    "        logits, sorted_tokens = torch.sort(logits, descending=True, stable=True)\n",
    "\n",
    "        next_token_id = sorted_tokens.gather(-1, ranks.cuda()[:, idx].unsqueeze(-1))\n",
    "\n",
    "        generated_ids = torch.cat([generated_ids.cuda(), next_token_id], dim=1)\n",
    "output = generated_ids[:, 1:].flatten()\n",
    "generated_text = tokenizer.decode(output[:-pad_len], skip_special_tokens=True)\n",
    "print(\"Final generated sequence:\")\n",
    "print(generated_text)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([[tokenizer.bos_token_id]]*ranks.shape[0], device=tokens.device)\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([[tokenizer.bos_token_id]]*ranks.shape[0], device=tokens.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    past_key_values = None\n",
    "    for idx in range(CONTEXT_SIZE):\n",
    "        print(f'\\r{idx}/{CONTEXT_SIZE}', end='')\n",
    "        output = llm(input_ids.cuda(), past_key_values=past_key_values, top_k=1)\n",
    "        past_key_values = output.past_key_values\n",
    "\n",
    "        logits = output.logits[:, -1, :]  # shape: (n_chunks, vocab)\n",
    "        logits, sorted_tokens = torch.sort(logits, descending=True)\n",
    "\n",
    "        next_token_id = sorted_tokens.gather(-1, ranks.cuda()[:, idx].unsqueeze(-1))\n",
    "\n",
    "        input_ids = torch.cat([input_ids.cuda(), next_token_id], dim=1)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in zip(\n",
    "    generated_text.split(\":\"),\n",
    "    s.split(\":\")\n",
    "):\n",
    "    print(float(a) - float(b))\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(44)\n",
    "probas = torch.rand(4000, 50000)\n",
    "probas /= probas.sum(1)[:, None]\n",
    "targets = torch.randint(0, 50000, (4000,))\n",
    "\n",
    "def argsort_solution(x, targets):\n",
    "    sort = torch.argsort(-x, dim=1, stable=True)\n",
    "    return torch.where(sort == targets[:, None])[1]\n",
    "\n",
    "def get_rank(x, indices):\n",
    "    # count the strictly the number of greater values\n",
    "    vals = x.gather(-1, indices[..., None]).squeeze(-1)\n",
    "    n_gt = (x > vals[:, None]).sum(-1)\n",
    "\n",
    "    # \"mimic\" stable sorting\n",
    "    eq = (x == vals[:, None])#.sum(-1)\n",
    "    mask = torch.arange(x.shape[-1]).unsqueeze(0) < indices.unsqueeze(1)\n",
    "    n_eq = (eq*mask).sum(-1)\n",
    "\n",
    "    return n_gt + n_eq\n",
    "\n",
    "a = argsort_solution(probas, targets)\n",
    "b = get_rank(probas, targets)\n",
    "\n",
    "for x, y, in zip(a, b):\n",
    "    if x != y:\n",
    "        print(x, y, x - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(targets == targets.max()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.tensor([\n",
    "    [4, 3, 5, 4, 7],\n",
    "    [4, 4, 5, 7, 4],\n",
    "])\n",
    "idx = torch.tensor([3, 4])\n",
    "val = v.gather(-1, idx[..., None]).squeeze(-1)\n",
    "print(\"idx\", idx, \"val\", val)\n",
    "\n",
    "gt = (v > val[:, None]).sum(-1)\n",
    "print(\"gt\", gt)\n",
    "\n",
    "eq = (v == val[:, None])\n",
    "print(\"eq\", eq.long())\n",
    "\n",
    "mask = torch.arange(v.shape[-1]).unsqueeze(0) < idx.unsqueeze(1)\n",
    "print(mask.int())\n",
    "n_eq = (eq*mask).sum(-1)\n",
    "print(\"n_eq\", n_eq)\n",
    "\n",
    "rank = gt + n_eq\n",
    "print(\"rank\", rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
